{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "17aUKkNJqTDZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweMlOB0L4mG"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-lgwhE1N5_7",
        "outputId": "13a8854d-22aa-4dcd-ae98-d506d8a98fb7"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "//MACRO TO DEBUG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivrxLaYOYPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61753e11-3b46-4dab-e975-016e1d376710"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_stuff.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "## Matrix Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A97U902HMog4",
        "outputId": "d5bb43e6-2185-46c3-bdf9-8ec1b992c8c5"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include <stddef.h>\n",
        "\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    size_t cols;\n",
        "    size_t rows;\n",
        "} fmatrix;\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major,\n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        "\n",
        "/* Access element (i,j) of matrix mat */\n",
        "#define getfm(mat,i,j) (mat.data[IDX2C(i,j,mat.rows)])\n",
        "\n",
        "\n",
        "size_t fmatrix_elements(fmatrix mat);\n",
        "size_t fmatrix_size(fmatrix mat);\n",
        "void fmatrix_init(fmatrix mat, float f);\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        "void fmatrix_assert();\n",
        "\n",
        "fmatrix fmatrix_create_on_host(size_t rows, size_t cols);\n",
        "fmatrix fmatrix_create_on_device(size_t rows, size_t cols);\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host.\n",
        " *  If nb<0, print all rows.\n",
        " */\n",
        "void fmatrix_host_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device.\n",
        " *  If nb<0, print all rows.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwZ36ifWQ-d",
        "outputId": "e5c819bb-ccfd-45d7-dd06-39c6d012d234"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "size_t fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "size_t fmatrix_size(fmatrix mat) {\n",
        "     return fmatrix_elements(mat) * sizeof(float);\n",
        "}\n",
        "\n",
        "void fmatrix_init(fmatrix mat, float f) {\n",
        "    for (int i = 0; i < mat.rows; i++){\n",
        "        for (int j = 0; j < mat.cols; j++){\n",
        "            mat.data[IDX2C(i,j,mat.rows)] = f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "fmatrix fmatrix_create_on_host(size_t rows, size_t cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat));\n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(size_t rows, size_t cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk(\n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat))\n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_device.data, mat_host.data,\n",
        "                   fmatrix_size(mat_host),\n",
        "                   cudaMemcpyHostToDevice\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,\n",
        "                   fmatrix_size(mat_device),\n",
        "                   cudaMemcpyDeviceToHost\n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);\n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_host_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_create_on_host(mat.rows, mat.cols);\n",
        "   fmatrix_data_to_host(tmp, mat);\n",
        "   fmatrix_host_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fmatrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM266RRGjwUH"
      },
      "source": [
        "## Matrix Math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNjf6dkCfh9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2294b1dc-8836-4374-f724-3c62b750d945"
      },
      "source": [
        "%%writefile sgemm.cuh\n",
        "#ifndef sgemm_H\n",
        "#define sgemm_H\n",
        "\n",
        "#include <string>\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "void mat_mul(fmatrix A, fmatrix B, fmatrix C, std::string arg);\n",
        "\n",
        "#endif"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sgemm.cuh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwAnQevYMQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6efaf3e-6122-485d-8b24-0d4131fe24bc"
      },
      "source": [
        "%%writefile sgemm.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string>\n",
        "#include <time.h>\n",
        "#include <math.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include \"cublas_v2.h\"\n",
        "\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"sgemm.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "static cublasHandle_t handle;\n",
        "// static int cublas_init = 0;\n",
        "\n",
        "/* basic matrix multiplication C = alpha*A*B + beta*C on host as reference for the speedup */\n",
        "void matrixMultiplication_basic_host(float alpha, fmatrix A, fmatrix B, float beta, fmatrix C) {\n",
        "  float tmp = 0;\n",
        "  for (int i = 0; i<A.rows; i++){\n",
        "    for (int j = 0; j<B.cols; j++){\n",
        "      for (int k = 0; k<A.cols; k++){\n",
        "        tmp += alpha * getfm(A,i, k) * getfm(B, k, j);\n",
        "      }\n",
        "      getfm(C, i, j) = beta * getfm(C, i, j) + tmp;\n",
        "      tmp = 0;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "/* TODO : 3 different versions of matrix multiplication C = alpha*A*B + beta*C on device */\n",
        "\n",
        "\n",
        "__global__\n",
        "void matmul_basic_kernel(float alpha, float *A, float *B, float beta, float *C, int nb_ColA, int nb_ColB, int nb_LigneA, int nb_LigneB) {\n",
        "  /* TODO */\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < nb_LigneA && col < nb_ColB) {\n",
        "    float tmp = 0;\n",
        "    for (int k = 0; k < nb_ColA; k++) {\n",
        "      tmp += A[row * nb_ColA + k] * B[k * nb_ColB + col];\n",
        "    }\n",
        "    C[row * nb_ColB + col] = alpha * tmp + beta * C[row * nb_ColB + col];\n",
        "    tmp = 0;\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "}\n",
        "void matrixMultiplication_basic(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C) {\n",
        "  // TODO - declaration of dimGrid and dimBlock\n",
        "\n",
        "  dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 dimGrid((d_B.cols + TILE_WIDTH - 1) / TILE_WIDTH,\n",
        "                 (d_A.cols + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "  matmul_basic_kernel <<< dimGrid, dimBlock >>> (alpha, d_A.data, d_B.data, beta, d_C.data, d_A.cols, d_B.cols, d_A.rows, d_B.rows);\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "/**********************/\n",
        "__global__\n",
        "void matmul_tiled_kernel(float alpha, float *A, float *B, float beta, float *C, int nb_ColA, int nb_ColB, int nb_LigneA, int nb_LigneB){\n",
        "  /* TODO */\n",
        "  // Allocate shared memory for tiles from A and B.\n",
        "  __shared__ float tileA[TILE_WIDTH][TILE_WIDTH];\n",
        "  __shared__ float tileB[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "  // Thread indices within the block\n",
        "  int tx = threadIdx.x;\n",
        "  int ty = threadIdx.y;\n",
        "\n",
        "  // Row and column of the C element to compute\n",
        "  int row = blockIdx.y * TILE_WIDTH + ty;\n",
        "  int col = blockIdx.x * TILE_WIDTH + tx;\n",
        "\n",
        "  float tmp = 0;\n",
        "\n",
        "  // Loop over all tiles needed to compute the C element.\n",
        "  // Note: The loop variable t indexes the tiles.\n",
        "  for (int t = 0; t < (nb_ColA + TILE_WIDTH - 1) / TILE_WIDTH; t++) {\n",
        "      // Load element from A into shared memory\n",
        "      if (row < nb_LigneA && t * TILE_WIDTH + tx < nb_ColA)\n",
        "          tileA[ty][tx] = A[row * nb_ColA + t * TILE_WIDTH + tx];\n",
        "      else\n",
        "          tileA[ty][tx] = 0.;\n",
        "\n",
        "      // Load element from B into shared memory\n",
        "      if (col < nb_ColB && t * TILE_WIDTH + ty < nb_LigneB)\n",
        "          tileB[ty][tx] = B[(t * TILE_WIDTH + ty) * nb_ColB + col];\n",
        "      else\n",
        "          tileB[ty][tx] = 0.;\n",
        "\n",
        "      // Synchronize to ensure the tile is fully loaded\n",
        "      __syncthreads();\n",
        "\n",
        "      // Multiply the two tiles together\n",
        "      for (int k = 0; k < TILE_WIDTH; k++) {\n",
        "          tmp += tileA[ty][k] * tileB[k][tx];\n",
        "      }\n",
        "      // Synchronize before loading the next tile\n",
        "      __syncthreads();\n",
        "  }\n",
        "\n",
        "  // Write the result to C, applying alpha and beta scaling\n",
        "  if (row < nb_LigneA && col < nb_ColB)\n",
        "      C[row * nb_ColB + col] = alpha * tmp + beta * C[row * nb_ColB + col];\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "void matrixMultiplication_tiled(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C){\n",
        "  // TODO - declaration of dimGrid and dimBlock\n",
        "  dim3 dimBlock(TILE_WIDTH, TILE_WIDTH);\n",
        "  dim3 dimGrid(( d_B.cols + TILE_WIDTH - 1) / TILE_WIDTH,\n",
        "                (d_A.rows + TILE_WIDTH - 1) / TILE_WIDTH);\n",
        "\n",
        "  matmul_tiled_kernel <<< dimGrid, dimBlock >>> (alpha, d_A.data, d_B.data, beta, d_C.data, d_A.cols, d_B.cols, d_A.rows, d_B.rows);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/**********************/\n",
        "void matrixMultiplication_cublas(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C){\n",
        "  /* TODO */\n",
        "\n",
        "\n",
        "  // We are launching our code from C proramm, that uses the row-major representation by default, cublas uses\n",
        "  // the column-major, so we must necessary transposes matrices before passing them to the cuBlas-handl\n",
        "\n",
        "\n",
        "\n",
        "  cublasCreate(&handle);\n",
        "\n",
        "  cublasStatus_t stat = cublasSgemm(handle,\n",
        "                                    CUBLAS_OP_T, CUBLAS_OP_T,\n",
        "                                    d_B.cols, d_A.rows, d_A.rows,\n",
        "                                    &alpha,\n",
        "                                    d_B.data, d_B.cols,\n",
        "                                    d_A.data, d_A.cols,\n",
        "                                    &beta,\n",
        "                                    d_C.data, d_B.cols);\n",
        "  if (stat != CUBLAS_STATUS_SUCCESS) {\n",
        "      printf(\"cuBLAS matrix multiplication failed\\n\");\n",
        "  }\n",
        "\n",
        "  cublasDestroy(handle);\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/*MAIN SGEMM*/\n",
        "void gen_mat_mul(float alpha, fmatrix A, fmatrix B, float beta, fmatrix C, std::string arg){\n",
        "    if (arg == \"cpu\"){\n",
        "        matrixMultiplication_basic_host(alpha, A, B, beta, C);\n",
        "    } else {\n",
        "      /* kernel function*/\n",
        "      if (arg == \"gpu_basic\"){\n",
        "          matrixMultiplication_basic(alpha, A, B, beta, C);\n",
        "\n",
        "      } else if (arg == \"gpu_tiled\"){\n",
        "          matrixMultiplication_tiled(alpha, A, B, beta, C);\n",
        "\n",
        "      } else if (arg == \"gpu_cublas\"){\n",
        "         matrixMultiplication_cublas(alpha, A, B, beta, C);\n",
        "\n",
        "      } else{\n",
        "          printf(\"Matrix Multiplication argument is Wrong\");\n",
        "          exit(0);\n",
        "      }\n",
        "      // wait for everything to finish\n",
        "      device_synchronize();\n",
        "    }\n",
        "}\n",
        "\n",
        "void mat_mul(fmatrix A, fmatrix B, fmatrix C, std::string arg){\n",
        " gen_mat_mul(1.0, A, B, 0.0, C, arg);\n",
        "}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sgemm.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnpSu2wH2ooy"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWEplkuA2Ygf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbb3089-c265-4fa6-ffcb-24b16d211df4"
      },
      "source": [
        "%%writefile main.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"sgemm.cuh\"\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "#define SIZE 40\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  /* Allocate and initialize data on host */\n",
        "  fmatrix A = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(A, 1.0);\n",
        "  fmatrix B = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(B, 2.0);\n",
        "  fmatrix C = fmatrix_create_on_host(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix_init(C, 0.0);\n",
        "\n",
        "  /* Allocate data on device */\n",
        "  fmatrix d_A = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix d_B = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "  fmatrix d_C = fmatrix_create_on_device(TILE_WIDTH * SIZE, TILE_WIDTH * SIZE);\n",
        "\n",
        "  /* Transfer A and B on device */\n",
        "  fmatrix_data_to_device(A, d_A);\n",
        "  fmatrix_data_to_device(B, d_B);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  clock_t start, end;\n",
        "  float cpu_time_used;\n",
        "\n",
        "  /* Start calculation \"cpu\", \"gpu_basic\", \"gpu_tiled\", \"gpu_cublas\" */\n",
        "  /************** \"cpu\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(A, B, C, \"cpu\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"Time taken by CPU in milliseconds: %.2f\\n\", cpu_time_used);\n",
        "\n",
        "\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "\n",
        "  /************** \"gpu_basic\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_basic\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU basic matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "\n",
        " /************** \"gpu_tiled\" *******************/\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_tiled\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU tiled matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "\n",
        "  /************** \"gpu_cublas\" *******************/\n",
        "  for(int warmup = 0; warmup < 5; warmup++){\n",
        "    mat_mul(d_A, d_B, d_C, \"gpu_cublas\");\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  start = clock();\n",
        "  mat_mul(d_A, d_B, d_C, \"gpu_cublas\");\n",
        "  end = clock();\n",
        "  cpu_time_used = ((double) (end - start)) * 1000 / CLOCKS_PER_SEC;\n",
        "  printf(\"GPU cuBLAS matrix multiplication in milliseconcs : %.2f\\n\", cpu_time_used);\n",
        "\n",
        "  /* Retrieve the result */\n",
        "  fmatrix_data_to_host(C, d_C);\n",
        "  /* Result correctness */\n",
        "  {\n",
        "    float maxError = 0.0f;\n",
        "    for (int i = 0; i < TILE_WIDTH * SIZE; i++){\n",
        "      for (int j = 0; j < TILE_WIDTH * SIZE; j++){\n",
        "        maxError = max(maxError, abs(getfm(C,i,j)- 2*TILE_WIDTH * SIZE));\n",
        "      }\n",
        "    }\n",
        "    printf(\"Max error: %f\\n\", maxError);\n",
        "  }\n",
        "  fmatrix_init(C, 0.0);\n",
        "  fmatrix_data_to_device(C, d_C);\n",
        "\n",
        "  /* Free */\n",
        "  fmatrix_free_on_host(&A);\n",
        "  fmatrix_free_on_host(&B);\n",
        "  fmatrix_free_on_host(&C);\n",
        "  fmatrix_free_on_device(&d_A);\n",
        "  fmatrix_free_on_device(&d_B);\n",
        "  fmatrix_free_on_device(&d_C);\n",
        "}\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrATC8s9LsDw"
      },
      "source": [
        "# Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z52xd0NMRKXb"
      },
      "source": [
        "!nvcc -lcublas sgemm.cu  fmatrix.cu  cuda_stuff.cu main.cu -arch=sm_75 -o main"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVqTfXcLvPr"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_D8hNmXwi0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b11e1f9-35ff-42f4-cbaf-d0c13455e12e"
      },
      "source": [
        "! ./main"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by CPU in milliseconds: 16566.52\n",
            "Max error: 0.000000\n",
            "GPU basic matrix multiplication in milliseconcs : 13.36\n",
            "Max error: 0.000000\n",
            "GPU tiled matrix multiplication in milliseconcs : 10.32\n",
            "Max error: 0.000000\n",
            "GPU cuBLAS matrix multiplication in milliseconcs : 2.00\n",
            "Max error: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aUKkNJqTDZ"
      },
      "source": [
        "# Debugging\n",
        "Compile with debugging info on the host (`-g`) and device (`-G`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfLGo1UrMq9"
      },
      "source": [
        "!nvcc -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver sgemm.cu fmatrix.cu cuda_stuff.cu main.cu"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkuaGO10rRm9"
      },
      "source": [
        "Run the debugger cuda-gdb, stopping at the first error that is detected. Shows first the call stack on the GPU, the values of local variables, then the call stack on the host (thread 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ8nAtzGTRgH",
        "outputId": "e2c787b4-dedb-4487-9fdc-ae5f66122bba"
      },
      "source": [
        "! printf \"set cuda api_failures stop\\ncatch throw\\nr UNIT\\nbt\\ninfo locals\\nthread 1\\nbt\\n\" > tmp.txt\n",
        "! cat tmp.txt\n",
        "! cuda-gdb -batch -x tmp.txt ./a.out"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "set cuda api_failures stop\n",
            "catch throw\n",
            "r UNIT\n",
            "bt\n",
            "info locals\n",
            "thread 1\n",
            "bt\n",
            "Catchpoint 1 (throw)\n",
            "[Thread debugging using libthread_db enabled]\n",
            "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
            "[New Thread 0x7fffd013c000 (LWP 7846)]\n",
            "[New Thread 0x7fffcedff000 (LWP 7847)]\n",
            "[Detaching after fork from child process 7848]\n",
            "[New Thread 0x7fffcd2e2000 (LWP 7853)]\n",
            "Time taken by CPU in milliseconds: 17413.79\n",
            "Max error: 0.000000\n",
            "Cuda API error detected: cudaLaunchKernel returned (0xde)\n",
            "#0  0x00007fffd15ad970 in cudbgReportDriverApiError () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#1  0x00007fffd182b32b in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#2  0x00007fffcef54ba7 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#3  0x00007fffcef31b2e in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#4  0x00007fffcef42fda in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#5  0x00007fffcef280d7 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#6  0x00007fffcf09e526 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#7  0x00007fffd1842066 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#8  0x00005555555cb9d8 in cudaLaunchKernel ()\n",
            "#9  0x0000555555560a7e in cudaLaunchKernel<char> (func=0x555555560652 <matmul_basic_kernel(float, float*, float*, float, float*, int, int, int, int)> \"\\363\\017\\036\\372UH\\211\\345H\\203\\3540\\363\\017\\021E\\374H\\211}\\360H\\211u\\350\\363\\017\\021M\\370H\\211U\\340\\211M\\334D\\211E\\330D\\211M\\324D\\213M\\324D\\213U؋M\\334H\\213U\\340\\363\\017\\020E\\370H\\213u\\350H\\213}\\360\\213E\\374H\\203\\354\\bD\\213E\\020APE\\211\\320\\017(\\310f\\017n\\300\\350\\225\\375\\377\\377H\\203\\304\\020\\220\\311\\303\\363\\017\\036\\372UH\\211\\345H\\201\", <incomplete sequence \\354\\260>, gridDim=..., blockDim=..., args=0x7fffffffd2a0, sharedMem=0, stream=0x0) at /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:216\n",
            "#10 0x0000555555560638 in __device_stub__Z19matmul_basic_kernelfPfS_fS_iiii (__par0=1, __par1=0x7fffb3200000, __par2=0x7fffb5200000, __par3=0, __par4=0x7fffb7600000, __par5=1280, __par6=1280, __par7=1280, __par8=1280) at /tmp/tmpxft_00001e01_00000000-6_sgemm.cudafe1.stub.c:14\n",
            "#11 0x00005555555606b7 in matmul_basic_kernel (__cuda_0=1, __cuda_1=0x7fffb3200000, __cuda_2=0x7fffb5200000, __cuda_3=0, __cuda_4=0x7fffb7600000, __cuda_5=1280, __cuda_6=1280, __cuda_7=1280, __cuda_8=1280) at /content/sgemm.cu:40\n",
            "#12 0x000055555555ffd6 in matrixMultiplication_basic (alpha=1, d_A=..., d_B=..., beta=0, d_C=...) at /content/sgemm.cu:64\n",
            "#13 0x0000555555560260 in gen_mat_mul (alpha=1, A=..., B=..., beta=0, C=..., arg=...) at /content/sgemm.cu:173\n",
            "#14 0x00005555555603a1 in mat_mul (A=..., B=..., C=..., arg=...) at /content/sgemm.cu:191\n",
            "#15 0x000055555556196c in main () at /content/main.cu:56\n",
            "No symbol table info available.\n",
            "[Switching to thread 1 (Thread 0x7ffff14c1000 (LWP 7842))]\n",
            "#0  0x00007fffd15ad970 in cudbgReportDriverApiError () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#0  0x00007fffd15ad970 in cudbgReportDriverApiError () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#1  0x00007fffd182b32b in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#2  0x00007fffcef54ba7 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#3  0x00007fffcef31b2e in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#4  0x00007fffcef42fda in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#5  0x00007fffcef280d7 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#6  0x00007fffcf09e526 in ?? () from /usr/lib64-nvidia/libcudadebugger.so.1\n",
            "#7  0x00007fffd1842066 in ?? () from /usr/lib64-nvidia/libcuda.so.1\n",
            "#8  0x00005555555cb9d8 in cudaLaunchKernel ()\n",
            "#9  0x0000555555560a7e in cudaLaunchKernel<char> (func=0x555555560652 <matmul_basic_kernel(float, float*, float*, float, float*, int, int, int, int)> \"\\363\\017\\036\\372UH\\211\\345H\\203\\3540\\363\\017\\021E\\374H\\211}\\360H\\211u\\350\\363\\017\\021M\\370H\\211U\\340\\211M\\334D\\211E\\330D\\211M\\324D\\213M\\324D\\213U؋M\\334H\\213U\\340\\363\\017\\020E\\370H\\213u\\350H\\213}\\360\\213E\\374H\\203\\354\\bD\\213E\\020APE\\211\\320\\017(\\310f\\017n\\300\\350\\225\\375\\377\\377H\\203\\304\\020\\220\\311\\303\\363\\017\\036\\372UH\\211\\345H\\201\", <incomplete sequence \\354\\260>, gridDim=..., blockDim=..., args=0x7fffffffd2a0, sharedMem=0, stream=0x0) at /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:216\n",
            "#10 0x0000555555560638 in __device_stub__Z19matmul_basic_kernelfPfS_fS_iiii (__par0=1, __par1=0x7fffb3200000, __par2=0x7fffb5200000, __par3=0, __par4=0x7fffb7600000, __par5=1280, __par6=1280, __par7=1280, __par8=1280) at /tmp/tmpxft_00001e01_00000000-6_sgemm.cudafe1.stub.c:14\n",
            "#11 0x00005555555606b7 in matmul_basic_kernel (__cuda_0=1, __cuda_1=0x7fffb3200000, __cuda_2=0x7fffb5200000, __cuda_3=0, __cuda_4=0x7fffb7600000, __cuda_5=1280, __cuda_6=1280, __cuda_7=1280, __cuda_8=1280) at /content/sgemm.cu:40\n",
            "#12 0x000055555555ffd6 in matrixMultiplication_basic (alpha=1, d_A=..., d_B=..., beta=0, d_C=...) at /content/sgemm.cu:64\n",
            "#13 0x0000555555560260 in gen_mat_mul (alpha=1, A=..., B=..., beta=0, C=..., arg=...) at /content/sgemm.cu:173\n",
            "#14 0x00005555555603a1 in mat_mul (A=..., B=..., C=..., arg=...) at /content/sgemm.cu:191\n",
            "#15 0x000055555556196c in main () at /content/main.cu:56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4CAfp3TIQGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}